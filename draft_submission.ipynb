{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/facenetpytorchvggface2/facenet_pytorch-2.0.1-py3-none-any.whl","execution_count":1,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/facenetpytorchvggface2/facenet_pytorch-2.0.1-py3-none-any.whl\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from facenet-pytorch==2.0.1) (2.22.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from facenet-pytorch==2.0.1) (1.17.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (1.24.2)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (2019.9.11)\nInstalling collected packages: facenet-pytorch\nSuccessfully installed facenet-pytorch-2.0.1\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport cv2\nimport gc\nimport glob\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport time\nimport tensorflow as tf\nimport torch\n\nfrom facenet_pytorch import MTCNN\nfrom PIL import Image\nfrom tensorflow.keras.models import load_model\n\nprint('TF version:', tf.__version__)\nprint('Torch version:', torch.__version__)\n","execution_count":2,"outputs":[{"output_type":"stream","text":"TF version: 2.1.0-rc0\nTorch version: 1.3.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n\nMETA_DATA = \"metadata.json\"\nMARGIN = 16\nMAX_DETECTION_SIZE = 960\nSEQ_LEN = 30\nTRAIN_FACE_SIZE = 224\nTRAIN_FRAME_COUNT = 32\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(device)\ndetector = MTCNN(device=device, margin=MARGIN, min_face_size=20, post_process=False, keep_all=False, select_largest=False)\n\nINPUT_DIR = '/kaggle/input/deepfake-detection-challenge'\ninput_dir = INPUT_DIR\n","execution_count":3,"outputs":[{"output_type":"stream","text":"1 Physical GPUs, 1 Logical GPUs\ncuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_vid(video_path, max_detection_size, max_frame_count):\n    vidcap = cv2.VideoCapture(video_path)\n    frame_num = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print('cv2.CAP_PROP_FRAME_COUNT: {}'.format(frame_num))\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)) # float\n    height = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float\n\n    skip_n = max(math.floor(frame_num / max_frame_count), 0)\n    max_dimension = max(width, height)\n    img_scale = 1.0\n    if max_dimension > max_detection_size:\n        img_scale = max_detection_size / max_dimension\n    print('Skipping %1.1f frames, scaling: %1.4f' % (skip_n, img_scale))\n\n    imrs = []\n    imgs = []\n    count = 0\n\n    #TODO make this robust to video reading errors\n    while True:\n        # success, im = vidcap.read()\n        success = vidcap.grab()\n        if success:\n            if count % (skip_n+1) == 0:\n                success, im = vidcap.retrieve()\n                if success:\n                    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n                    if img_scale < 1.0:\n                        imr = cv2.resize(im, (int(im.shape[1] * img_scale), int(im.shape[0] * img_scale)))\n                    else:\n                        imr = im\n                    imgs.append(im)\n                    imrs.append(imr)\n            count += 1\n        else:\n            break\n\n    vidcap.release()\n    return imgs, imrs, img_scale","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_faces_bbox(detector, originals, images, batch_size, img_scale, face_size):\n    faces = []\n    face_locations = []\n    for lb in np.arange(0, len(images), batch_size):\n        imgs_pil = [Image.fromarray(image) for image in images[lb:lb+batch_size]]\n        frames_boxes, frames_confidences = detector.detect(imgs_pil, landmarks=False)\n\n        #TODO figure out a way to use all faces.\n        for batch_idx, (frame_boxes, frame_confidences) in enumerate(zip(frames_boxes, frames_confidences), 0):\n            # print(frame_boxes)\n            original = originals[lb + batch_idx]\n            # print('Original size {}, used for detection size {}'.format(original.shape, images[lb+batch_idx].shape))\n            if ((frame_boxes is not None) and (len(frame_boxes) > 0)):\n                selected_face_box = frame_boxes[0]\n                # Distance logic is needed to weed out face-like artifact detection\n                if len(face_locations) > 0:\n                    min_distance = float('inf')\n                    last_face_box = face_locations[-1]\n                    for j, (face_box, confidence) in enumerate(zip(frame_boxes, frame_confidences), 0):\n                        distance = (face_box[0] - last_face_box[0])**2 + (face_box[1] - last_face_box[1])**2\n                        # print(distance)\n                        if distance < min_distance:\n                            min_distance = distance\n                            selected_face_box = face_box\n                \n                face_locations.append(selected_face_box)\n                (x,y,w,h) = (\n                    max(int(selected_face_box[0] / img_scale) - MARGIN, 0),\n                    max(int(selected_face_box[1] / img_scale) - MARGIN, 0),\n                    int((selected_face_box[2]-selected_face_box[0]) / img_scale) + 2*MARGIN,\n                    int((selected_face_box[3]-selected_face_box[1]) / img_scale) + 2*MARGIN\n                )\n                face_extract = original[y:y+h, x:x+w].copy() # Without copy() memory leak with GPU\n                face_extract = cv2.resize(face_extract, (face_size, face_size))\n                faces.append(face_extract)\n\n    return faces","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras as keras\nimport tensorflow.keras.backend as K\n\nclass SeqWeightedAttention(keras.layers.Layer):\n    r\"\"\"Y = \\text{softmax}(XW + b) X\n\n    See: https://arxiv.org/pdf/1708.00524.pdf\n    \"\"\"\n\n    def __init__(self, use_bias=True, return_attention=False, **kwargs):\n        super(SeqWeightedAttention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.use_bias = use_bias\n        self.return_attention = return_attention\n        self.W, self.b = None, None\n\n    def get_config(self):\n        config = {\n            'use_bias': self.use_bias,\n            'return_attention': self.return_attention,\n        }\n        base_config = super(SeqWeightedAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        self.W = self.add_weight(shape=(int(input_shape[2]), 1),\n                                 name='{}_W'.format(self.name),\n                                 initializer=keras.initializers.get('uniform'))\n        if self.use_bias:\n            self.b = self.add_weight(shape=(1,),\n                                     name='{}_b'.format(self.name),\n                                     initializer=keras.initializers.get('zeros'))\n        super(SeqWeightedAttention, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        logits = K.dot(x, self.W)\n        if self.use_bias:\n            logits += self.b\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return input_shape[0], output_len\n\n    def compute_mask(self, _, input_mask=None):\n        if self.return_attention:\n            return [None, None]\n        return None\n\n    @staticmethod\n    def get_custom_objects():\n        return {'SeqWeightedAttention': SeqWeightedAttention}\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_one_sample_bbox(video_path, max_detection_size, max_frame_count, face_size):\n    start = time.time()\n    imgs, imrs, img_scale = parse_vid(video_path, max_detection_size, max_frame_count)\n    parsing = time.time() - start\n    faces = detect_faces_bbox(detector, imgs, imrs, 256, img_scale, face_size)\n    # print('faces: ', faces)\n    detection = time.time() - start - parsing\n    print('parsing: %.3f scale %f, detection: %.3f seconds' %(parsing, img_scale, detection))\n    return faces\n\ndef fraction_positives(y_true, y_pred):\n    return tf.keras.backend.mean(y_true)\n\n\ncustom_objs = {\n    'fraction_positives':fraction_positives,\n    'SeqWeightedAttention':SeqWeightedAttention,\n}\n\n\ndef run(file_list):\n\n    prediction_list = []\n    model = load_model('/kaggle/input/trained-model/final_model.h5', custom_objects=custom_objs)\n    len_file_list = len(file_list)\n    for i in range(len_file_list):\n        f_name = os.path.basename(file_list[i])\n        print('Now processing: {} {}/{}'.format(file_list[i], i, len_file_list))\n        faces = extract_one_sample_bbox(file_list[i], max_detection_size=MAX_DETECTION_SIZE, \n            max_frame_count=TRAIN_FRAME_COUNT, face_size=TRAIN_FACE_SIZE)\n        if len(faces) > 0:\n            pred_faces = np.zeros((SEQ_LEN, TRAIN_FACE_SIZE, TRAIN_FACE_SIZE, 3), dtype=np.float32)\n            mask = np.zeros(SEQ_LEN, dtype=np.float32)\n            for k in range(len(faces)):\n                pred_faces[k] = faces[k]\n                mask[k] = 1.0\n            pred_faces = pred_faces / 127.5 - 1.0\n            prediction = model.predict(\n                [np.expand_dims(pred_faces, axis=0), np.expand_dims(mask, axis=0)])\n        else:\n            prediction = 0.5\n        prediction_list.append([f_name, prediction])\n        if i % 10 == 0:\n            gc.collect()\n    \n    del model\n    return prediction_list\n\ndef save_predictions(predictions):\n    with open('submission.csv', 'w') as sf:\n        sf.write('filename,label\\n')\n        for name, score in predictions:\n            sf.write('%s,%1.6f\\n' % (name, score))\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n\n    file_paths = glob.glob(os.path.join(input_dir, 'test_videos/*.mp4'))\n    test_files = [os.path.basename(x) for x in file_paths]\n\n    try:\n        submission = pd.read_csv(os.path.join(input_dir, 'sample_submission.csv'))\n        csvfileset = set(submission.filename)\n        listdirset = set(test_files)\n        print('Are identical filenames in csv and test dir? ', csvfileset == listdirset)\n        print('csvfileset - listdirset', csvfileset - listdirset)\n        print('listdirset - csvfileset', listdirset - csvfileset)\n        del submission, csvfileset, listdirset\n        gc.collect()\n    except:\n        pass\n\n    t0 = time.time()\n    predictions = run(file_paths)\n    save_predictions(predictions)\n    t1 = time.time()\n    print(\"Execution took: {}\".format(t1-t0))\n\n    del detector\n    torch.cuda.empty_cache()\n","execution_count":8,"outputs":[{"output_type":"stream","text":"Are identical filenames in csv and test dir?  True\ncsvfileset - listdirset set()\nlistdirset - csvfileset set()\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/ywauoonmlr.mp4 0/400\ncv2.CAP_PROP_FRAME_COUNT: 299\nSkipping 9.0 frames, scaling: 0.5000\nparsing: 2.864 scale 0.500000, detection: 1.453 seconds\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'gc' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-717b821bfa27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0msave_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-6e930747041b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprediction_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}