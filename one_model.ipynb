{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/facenetpytorchvggface2/facenet_pytorch-2.0.1-py3-none-any.whl","execution_count":2,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/facenetpytorchvggface2/facenet_pytorch-2.0.1-py3-none-any.whl\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from facenet-pytorch==2.0.1) (2.22.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from facenet-pytorch==2.0.1) (1.17.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (2019.9.11)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (1.24.2)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->facenet-pytorch==2.0.1) (3.0.4)\nInstalling collected packages: facenet-pytorch\nSuccessfully installed facenet-pytorch-2.0.1\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport cv2\nimport gc\nimport glob\nimport joblib\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport time\nimport tensorflow as tf\nimport torch\n\nfrom facenet_pytorch import MTCNN\nfrom PIL import Image\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.models import load_model\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as K\n\nprint('TF version:', tf.__version__)\nprint('Keras version:', tf.keras.__version__)\nprint('Torch version:', torch.__version__)\n","execution_count":3,"outputs":[{"output_type":"stream","text":"TF version: 2.1.0-rc0\nKeras version: 2.2.4-tf\nTorch version: 1.3.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n\nMETA_DATA = \"metadata.json\"\nMARGIN = 16\nMAX_DETECTION_SIZE = 960\nSEQ_LEN = 30\nTRAIN_FACE_SIZE = 256\nTRAIN_FRAME_COUNT = 32\nTRAIN_FPS = 3\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(device)\ndetector = MTCNN(device=device, margin=MARGIN, min_face_size=20, post_process=False, keep_all=False, select_largest=False)\n\nINPUT_DIR = '/kaggle/input/deepfake-detection-challenge'\ninput_dir = INPUT_DIR","execution_count":4,"outputs":[{"output_type":"stream","text":"1 Physical GPUs, 1 Logical GPUs\ncuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_vid(video_path, max_detection_size, max_frame_count, sample_fps):\n    vidcap = cv2.VideoCapture(video_path)\n    frame_num = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)) # float\n    height = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float\n\n    skip_n = max(math.floor(fps / sample_fps), 0)\n    max_dimension = max(width, height)\n    img_scale = 1.0\n    if max_dimension > max_detection_size:\n        img_scale = max_detection_size / max_dimension\n#     print('Frame count %d, skipping %1.1f frames, scaling: %1.4f' % (frame_num, skip_n, img_scale))\n\n    imrs = []\n    imgs = []\n    count = 0\n\n    for i in range(frame_num):\n        success = vidcap.grab()            \n        if success:\n            if i % (skip_n+1) == 0:\n                success, im = vidcap.retrieve()\n                if success:\n                    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n                    if img_scale < 1.0:\n                        imr = cv2.resize(im, (int(im.shape[1] * img_scale), int(im.shape[0] * img_scale)))\n                    else:\n                        imr = im\n                    imgs.append(im)\n                    imrs.append(imr)\n                    count += 1\n                    if count >= max_frame_count:\n                        break\n        else:\n            break\n\n    vidcap.release()\n    return imgs, imrs, img_scale","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def iou(bbox1, bbox2):\n    \"\"\"\n    Calculates the intersection-over-union of two bounding boxes.\n\n    Args:\n        bbox1 (numpy.array, list of floats): bounding box in format x1,y1,x2,y2.\n        bbox2 (numpy.array, list of floats): bounding box in format x1,y1,x2,y2.\n\n    Returns:\n        int: intersection-over-onion of bbox1, bbox2\n    \"\"\"\n\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n        return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection / size_union\n\n\ndef track_iou(detections, sigma_l, sigma_h, sigma_iou, t_min):\n    \"\"\"\n    Simple IOU based tracker.\n    See \"High-Speed Tracking-by-Detection Without Using Image Information by E. Bochinski, V. Eiselein, T. Sikora\" for\n    more information.\n\n    Args:\n         detections (list): list of detections per frame, usually generated by util.load_mot\n         sigma_l (float): low detection threshold.\n         sigma_h (float): high detection threshold.\n         sigma_iou (float): IOU threshold.\n         t_min (float): minimum track length in frames.\n\n    Returns:\n        list: list of tracks.\n    \"\"\"\n\n    tracks_active = []\n    tracks_finished = []\n\n    for frame_num, detections_frame in enumerate(detections, start=1):\n        # apply low threshold to detections\n        dets = [det for det in detections_frame if det['score'] >= sigma_l]\n\n        updated_tracks = []\n        for track in tracks_active:\n            if len(dets) > 0:\n                # get det with highest iou\n                best_match_index = 0\n                best_iou = 0\n                for i, det in enumerate(dets):\n                    candidate_iou = iou(track['bboxes'][-1], det['bbox'])\n                    if candidate_iou > best_iou:\n                        best_match_index = i\n                        best_iou = candidate_iou\n                if best_iou >= sigma_iou:\n                    best_match = dets[best_match_index]\n                    track['bboxes'].append(best_match['bbox'])\n                    track['max_score'] = max(track['max_score'], best_match['score'])\n\n                    updated_tracks.append(track)\n                    # print('best match: ', best_match)\n                    # remove from best matching detection from detections\n                    del dets[best_match_index]\n\n            # if track was not updated\n            if len(updated_tracks) == 0 or track is not updated_tracks[-1]:\n                # finish track when the conditions are met\n                if track['max_score'] >= sigma_h and len(track['bboxes']) >= t_min:\n                    tracks_finished.append(track)\n\n        # create new tracks\n        new_tracks = [{'bboxes': [det['bbox']], 'max_score': det['score'], 'start_frame': frame_num} for det in dets]\n        tracks_active = updated_tracks + new_tracks\n\n    # finish all remaining active tracks\n    tracks_finished += [track for track in tracks_active\n                        if track['max_score'] >= sigma_h and len(track['bboxes']) >= t_min]\n\n    return tracks_finished","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_faces_bbox(detector, originals, images, batch_size, img_scale, face_size):\n    faces = []\n    detections = []\n\n    for lb in np.arange(0, len(images), batch_size):\n        imgs_pil = [Image.fromarray(image) for image in images[lb:lb+batch_size]]\n        frames_boxes, frames_confidences = detector.detect(imgs_pil, landmarks=False)\n        if (frames_boxes is not None) and (len(frames_boxes) > 0):\n#             print(frames_boxes, frames_confidences)\n            for i in range(len(frames_boxes)):\n                if frames_boxes[i] is not None:\n                    boxes = []\n                    for box, confidence in zip(frames_boxes[i], frames_confidences[i]):\n                        boxes.append({'bbox': box, 'score':confidence})\n                    detections.append(boxes)\n#     print(detections)\n    tracks = track_iou(detections, 0.8, 0.9, 0.1, 10)\n    tracks.sort(key = lambda x:x['max_score'], reverse=True)\n#     print(tracks)\n    for track in tracks[:2]:\n        track_faces = []\n        for i, bbox in enumerate(track['bboxes']):\n            original = originals[track['start_frame'] + i - 1]\n            (x,y,w,h) = (\n                max(int(bbox[0] / img_scale) - MARGIN, 0),\n                max(int(bbox[1] / img_scale) - MARGIN, 0),\n                int((bbox[2]-bbox[0]) / img_scale) + 2*MARGIN,\n                int((bbox[3]-bbox[1]) / img_scale) + 2*MARGIN\n            )\n            face_extract = original[y:y+h, x:x+w].copy() # Without copy() memory leak with GPU\n            face_extract = cv2.resize(face_extract, (face_size, face_size))\n            track_faces.append(face_extract)\n        faces.append(track_faces)\n\n    del tracks\n    del detections\n\n    return faces","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_one_sample_bbox(video_path, max_detection_size, max_frame_count, face_size):\n    start = time.time()\n    imgs, imrs, img_scale = parse_vid(video_path, max_detection_size, max_frame_count, TRAIN_FPS)\n    parsing = time.time() - start\n    faces = detect_faces_bbox(detector, imgs, imrs, 128, img_scale, face_size)\n    del imgs, imrs\n    # print('faces: ', faces)\n    detection = time.time() - start - parsing\n#     print('parsing: %.3f scale %f, detection: %.3f seconds' %(parsing, img_scale, detection))\n    return faces\n\n\ndef fraction_positives(y_true, y_pred):\n    return tf.keras.backend.mean(y_true)\n\n\ncustom_objs = {\n    'fraction_positives':fraction_positives,\n}\n\n\ndef run(file_list):\n\n    prediction_list = []\n    # Note: Kaggle renames the model folder behind my back\n    model = load_model('/kaggle/input/featureextractormodel/one_model.h5', custom_objects=custom_objs)\n#     print(model.summary())\n    score_calibrator = joblib.load('/kaggle/input/featureextractormodel/score_calibration.pkl')\n    len_file_list = len(file_list)\n    for i in range(len_file_list):\n        f_name = os.path.basename(file_list[i])\n        prediction = 0.5\n        try:\n            faces = extract_one_sample_bbox(file_list[i], max_detection_size=MAX_DETECTION_SIZE, \n                max_frame_count=TRAIN_FRAME_COUNT, face_size=TRAIN_FACE_SIZE)\n            print('Now processing: {}, faces {} progress {}/{}'.format(file_list[i], len(faces), i, len_file_list))\n            if len(faces) > 0:\n                prediction_faces = [item for sublist in faces for item in sublist]\n                prediction_faces = np.array(prediction_faces, dtype=np.float32) / 255.0            \n#                 prediction_faces = np.array(prediction_faces, dtype=np.float32) / 127.5 - 1.0            \n                model_prediction = model.predict(prediction_faces).flatten()\n#                 print('model preds: ', model_prediction)\n                if len(model_prediction) > 0:\n                    prediction = model_prediction.mean()\n                    prediction = score_calibrator.predict_proba(prediction.reshape(-1, 1))[:,1]\n#                     prediction = np.percentile(model_prediction, 60)\n                else:\n                    print('Model gave no prediction!')\n\n    #                 fig = plt.figure(figsize=(20, 24))\n    #                 for i, frame_face in enumerate(faces[0]):\n    #                     ax = fig.add_subplot(5, 6, i+1)\n    #                     ax.axis('off')\n    #                     plt.imshow(frame_face)\n    #                 plt.tight_layout()\n    #                 plt.show()\n\n                del prediction_faces\n            del faces\n\n        except Exception as e:\n            print(e)\n\n        print('file: {}, prediction: {}'.format(f_name, prediction))\n        prediction_list.append([f_name, prediction])\n        if i % 10 == 0:  \n            gc.collect()\n    \n    del model\n    return prediction_list\n\n\ndef save_predictions(predictions):\n    with open('submission.csv', 'w') as sf:\n        sf.write('filename,label\\n')\n        for name, score in predictions:\n#             score = np.clip(score, 0.49, 0.999) # expected 0.482\n            sf.write('%s,%1.6f\\n' % (name, score))\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n\n    t0 = time.time()\n\n    file_paths = glob.glob(os.path.join(input_dir, 'test_videos/*.mp4'))\n    test_files = [os.path.basename(x) for x in file_paths]\n\n#     try:\n#         submission = pd.read_csv(os.path.join(input_dir, 'sample_submission.csv'))\n#         csvfileset = set(submission.filename)\n#         listdirset = set(test_files)\n#         print('Are identical filenames in csv and test dir? ', csvfileset == listdirset)\n#         print('csvfileset - listdirset', csvfileset - listdirset)\n#         print('listdirset - csvfileset', listdirset - csvfileset)\n#         del submission, csvfileset, listdirset\n#         gc.collect()\n#     except:\n#         pass\n\n    predictions = run(file_paths)\n#     print(predictions)\n    save_predictions(predictions)\n    t1 = time.time()\n    print(\"Execution took: {}\".format(t1-t0))\n\n    torch.cuda.empty_cache()\n\n","execution_count":9,"outputs":[{"output_type":"stream","text":"Now processing: /kaggle/input/deepfake-detection-challenge/test_videos/ayipraspbn.mp4, faces 1 progress 0/14\nfile: ayipraspbn.mp4, prediction: [0.96011298]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/aktnlyqpah.mp4, faces 1 progress 1/14\nfile: aktnlyqpah.mp4, prediction: [0.99216091]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/ajiyrjfyzp.mp4, faces 2 progress 2/14\nfile: ajiyrjfyzp.mp4, prediction: [0.92512255]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/ahjnxtiamx.mp4, faces 1 progress 3/14\nfile: ahjnxtiamx.mp4, prediction: [0.79684177]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/aqrsylrzgi.mp4, faces 1 progress 4/14\nfile: aqrsylrzgi.mp4, prediction: [0.97695181]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/adohdulfwb.mp4, faces 1 progress 5/14\nfile: adohdulfwb.mp4, prediction: [0.59070446]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/apvzjkvnwn.mp4, faces 1 progress 6/14\nfile: apvzjkvnwn.mp4, prediction: [0.92211983]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/apedduehoy.mp4, faces 1 progress 7/14\nfile: apedduehoy.mp4, prediction: [0.97788979]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/axfhbpkdlc.mp4, faces 1 progress 8/14\nfile: axfhbpkdlc.mp4, prediction: [0.99238971]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/aayfryxljh.mp4, faces 1 progress 9/14\nfile: aayfryxljh.mp4, prediction: [0.44593514]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/aomqqjipcp.mp4, faces 1 progress 10/14\nfile: aomqqjipcp.mp4, prediction: [0.99421842]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/aassnaulhq.mp4, faces 1 progress 11/14\nfile: aassnaulhq.mp4, prediction: [0.98650169]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/acazlolrpz.mp4, faces 2 progress 12/14\nfile: acazlolrpz.mp4, prediction: [0.94357923]\nNow processing: /kaggle/input/deepfake-detection-challenge/test_videos/alrtntfxtd.mp4, faces 1 progress 13/14\nfile: alrtntfxtd.mp4, prediction: [0.99150633]\nExecution took: 49.79974889755249\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}